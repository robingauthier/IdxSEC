


2 grandes phases dans le projet, la premiere correctement parser les 14A et 10K avec l'ownership info
la deuxieme, predire la float des different index providers.


todo : issue on sec master it needs to be redownloaded every day in fact.


etape 0 :
get the structure of the form

peut etre on doit aussi faire un modele pour detecter ou est la table/partie relevant a l'ownership





          CIK    Company Name Form Type  Date Filed                                     Filename         cik
4233  1018724  AMAZON COM INC   DEF 14A  2023-04-13  edgar/data/1018724/0001104659-23-044708.txt  0001018724
4234  1018724  AMAZON COM INC   DEFA14A  2023-04-13  edgar/data/1018724/0001104659-23-044710.txt  0001018724
4235  1018724  AMAZON COM INC   DEFA14A  2023-05-03  edgar/data/1018724/0001104659-23-055145.txt  0001018724


<a href="#txa509319_41" tabindex="18">Stock Ownership</a>
https://huggingface.co/chat/
https://neptune.ai/blog/knowledge-distillation

https://huggingface.co/tiiuae/falcon-180B
https://huggingface.co/mistralai/Mistral-7B-v0.1
https://huggingface.co/docs/transformers/model_doc/distilbert ::light
https://huggingface.co/tasks/question-answering
https://huggingface.co/microsoft/deberta-v2-xlarge-mnli


pour detecter la partie sur l'ownership :
- score pour detecter une table des matieres ?

BERT embedding on each element -> model to tag if yes or not.
https://pypi.org/project/sentence-transformers/

peut

About cleaning the HTML
https://www.kaggle.com/code/mohammadbolandraftar/nlp-text-pre-processing-how-to-clean-html-format



comment tagger le document ? en utilisant les tags href. c'est la table des contents



you might want to train a NER : Named Entity recognition
https://spacy.io/usage/training
spacy train config.cfg




TODO: faut aussi faire les share outstanding et les voting rights


https://aclanthology.org/2022.emnlp-industry.35.pdf : bof


https://medium.com/@alessandropaticchio/named-entity-recognition-from-scratch-e76b9b3affad

how is bert trained ??
next sentence prediction loss NSP
and masked language model

https://machinelearningmastery.com/a-brief-introduction-to-bert/#:~:text=A%20BERT%20model%20is%20trained,sentence%20prediction%20(NSP)%20simultaneously.&text=Each%20training%20sample%20for%20BERT,in%20the%20document%20or%20not.
Bert is a bi directional transformer


https://github.com/ShannonAI/mrc-for-flat-nested-ner/blob/master/models/classifier.py




https://github.com/Lightning-AI/pytorch-lightning


https://catalog.ldc.upenn.edu/LDC2013T19 : 	OntoNotes Release 5.0 ---> this is a serious tree..
English CoNLL 2003 :: https://medium.com/analytics-vidhya/ner-tensorflow-2-2-0-9f10dcf5a0a#:~:text=The%20CoNLL%2D2003%20data%20files,fourth%20the%20named%20entity%20tag.


here is explains how to train BERT :
https://medium.com/data-and-beyond/complete-guide-to-building-bert-model-from-sratch-3e6562228891
but clearly it is not advised to do so!!!

https://pytorchnlp.readthedocs.io/en/latest/index.html



https://rumn.medium.com/part-1-ultimate-guide-to-fine-tuning-in-pytorch-pre-trained-model-and-its-configuration-8990194b71e


FINE TUNING is the GO TO APPROACH !!!!

https://mccormickml.com/2019/07/22/BERT-fine-tuning/
we usually add some layers at the end of the model and that's it

In fact, the authors recommend only 2-4 epochs of training for fine-tuning BERT on a specific NLP task

Use Google Colab
MODEL_CLASSES = {
    'bert': (BertConfig, BertForSequenceClassification, BertTokenizer),
    'xlnet': (XLNetConfig, XLNetForSequenceClassification, XLNetTokenizer),
    'xlm': (XLMConfig, XLMForSequenceClassification, XLMTokenizer),
    'roberta': (RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer),
    'distilbert': (DistilBertConfig, DistilBertForSequenceClassification, DistilBertTokenizer),
    'albert': (AlbertConfig, AlbertForSequenceClassification, AlbertTokenizer)
}

Here is the current list of classes provided for fine-tuning:

BertModel
BertForPreTraining
BertForMaskedLM
BertForNextSentencePrediction
BertForSequenceClassification - The one weâ€™ll use.
BertForTokenClassification ----> this is what I need
BertForQuestionAnswering

https://www.kaggle.com/code/thanish/bert-for-token-classification-ner-tutorial

fine tuning of bert full script:
https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128
https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_ner.py
l

je ne pense pas que la NER sera la bonne solution mais les question reponses peut etre.
aussi je pense qu'il faut utiliser BERT uniquement a la fin pour verifier les resultats

j'ai besoin d'un classifier :
une table est-elle une ownership table ? ou une table de detail des ownership ou autre
une row est-elle un header ou du content ?

enfin faudra aussi in modele de classification des colonne name en N -> summary

sur chaque ligne de la table d'ownership on posera une question.
- est ce que c'est correcte que xxx est le nombre de shares que la personne a
- est-ce que y a des RU inclues dans ce nombre ?
- est-ce que cette ligne est inclue dans une autre ligne.


demain :
finir le content table section classifier
pareil pour les tables classifiers et
si ownership alors faut mettre combien de rows sont du header.
{'nheaders': 3,
'isOwnership':1,
'isDetailOwnership':0,
'uniqueContent':['All executive officers and directors as a group','Philipp Schindler'],
 'fname':'form14A_cik898174_asof20230413_0000898174-23-000081.txt'},
{'nheaders': 0,
'isOwnership':0,
'isDetailOwnership':1,
'uniqueContent':['Percentage total voting power represents voting power with respect '],
 'fname':'form14A_cik898174_asof20230413_0000898174-23-000081.txt'},

 faut aussi un header classifier :
 {'header':'Voting Shares Beneficially Owned | Class A Common Stock | Shares ',
  'category':'ClassA-Sh'
  },
  {'name':'Total Voting Power',
  'category':'TotalPct',
  },
  {'name':'Number of Shares',
  'category':'Sh'
  }
  {'name':'Common Stock',
  'context':'Common Stock|Exercicable in 60 Days | Total Common Stock',
  'category':'Sh'
  }
enfin, une fois qu'on a mis le commentaire sur la meme ligne que la table,
on va entrainer BERT pour des QA.
{'context':"Number of Shares: 324,503. Includes 15,843 shares which may be acquired upon exercise of options exercisable or vesting of RSUs within 60 days of April 1, 2023.",
'question':'how many of those shares are RSU?',
'answer':'15,843', # the SQUAD dataset has this : Answer: {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}
}

https://huggingface.co/learn/nlp-course/chapter3/3?fw=pt
https://ivan-corrales-solera.medium.com/bert-understanding-the-null-response-threshold-in-qa-systems-b6cf55a4b8ec



la categorisation des lignes de la table c'est un modele de NER qu'il faut train from scratch


https://huggingface.co/docs/transformers/perf_train_cpu
 python run_qa.py \
--model_name_or_path bert-base-uncased \
--dataset_name squad \
--do_train \
--do_eval \
--per_device_train_batch_size 12 \
--learning_rate 3e-5 \
--num_train_epochs 2 \
--max_seq_length 384 \
--doc_stride 128 \
--output_dir /tmp/debug_squad/ \
--use_ipex \
--bf16 --no_cuda

 python run_qa.py \
--model_name_or_path bert-base-uncased \
--dataset_name squad \
--do_train \
--do_eval \
--per_device_train_batch_size 12 \
--learning_rate 3e-5 \
--num_train_epochs 2 \
--max_seq_length 384 \
--doc_stride 128 \
--output_dir /tmp/debug_squad/ \
--overwrite_output_dir \
--bf16 --no_cuda




https://huggingface.co/spaces/HuggingFaceM4/screenshot2html
app sympa

