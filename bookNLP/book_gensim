bookof chris kuo
“BERTopic for legal documents”

Excerpt From: Chris Kuo. “The Handbook of NLP with Gensim”. Apple Books.


livre assez moyen !!!

Thomas Dop - Hands-On Natural Language Processing with PyTorch 1.x_
Build smart, AI-driven linguistic applications using deep learning
and NLP techniques-Packt Publishing Ltd (2020)


bow_vec = make_bow_vector(sentence, word_dict)   means bag of word vector
sklearn enables to compute the cosine similarity
cosine_similarity

sentence = "The big dog is sleeping on the bed"
token = nltk.word_tokenize(sentence)
nltk.pos_tag(token)


input_text = "<b> This text is in bold</br>, <i> This text is in italics </i>"
output_text = BeautifulSoup(input_text, "html.parser").get_text()
print('Input: ' + input_text)
print('Output: ' + output_text)

input_text = "This ,sentence.'' contains-£ no:: punctuation?"
output_text = re.sub(r'[^\w\s]', '', input_text)
print('Input: ' + input_text)
print('Output: ' + output_text)



def to_digit(digit):
i = inflect.engine() if digit.isdigit():
output = i.number_to_words(digit) else:
output = digit return output
input_text = ["1","two","3"]
output_text = [to_digit(x) for x in input_text] print('Input: ' + str(input_text)) print('Output: ' + str(output_text))

words = p.number_to_words(1234, andword="")
# "one thousand, two hundred thirty-four"
https://pypi.org/project/inflect/

in RNN use gradient clipping !
lstm much better than rnn!



class SentimentLSTM(nn.Module):
def __init__(self, n_vocab, n_embed, n_hidden, n_output, 0.8):
super().__init__()
self.n_vocab = n_vocab
self.n_layers = n_layers
self.n_hidden = n_hidden

self.embedding = nn.Embedding(n_vocab, n_embed)
self.lstm = nn.LSTM(n_embed, n_hidden, n_layers, batch_first = True, dropout = drop_p)
self.dropout = nn.Dropout(drop_p)
self.fc = nn.Linear(n_hidden, n_output)
self.sigmoid = nn.Sigmoid()

def forward (self, input_words):
embedded_words = self.embedding(input_words)
lstm_out, h = self.lstm(embedded_words)
lstm_out = self.dropout(lstm_out)
lstm_out = lstm_out.contiguous().view(-1,self.n_hidden)
fc_out = self.fc(lstm_out)
sigmoid_out = self.sigmoid(fc_out)
sigmoid_out = sigmoid_out.view(batch_size, -1)
sigmoid_last = sigmoid_out[:, -1]
return sigmoid_last, h


train_data = TensorDataset(train_x, train_y)
train_loader = DataLoader(train_data, batch_size = batch_size, shuffle = True)
clip = 5 in adam

for epoch in range(n_epochs):
h = net.init_hidden(batch_size)
for inputs, labels in train_loader: step += 1
net.zero_grad()
output, h = net(inputs)
loss = criterion(output.squeeze(), labels.float()) loss.backward() nn.utils.clip_grad_norm(net.parameters(), clip) optimizer.step()
torch.save(net.state_dict(), 'model.pkl')

While CNNs are more commonly used for classifying
 images for the reasons mentioned here, they have been shown to be effective at classifying text as well.


 Although CNNs and RNNs learn from text in different ways, they have both shown to be effective in
 text classification, and which one to use in any given situation depends on the nature of the task.

 That being said, CNNs for NLP have been proven to perform very well in some tasks, even though our
 language assumptions do not necessarily hold. Arguably, the main advantages of using CNNs for NLP
 are speed and efficiency. Convolutions can be easily implemented on GPUs, allowing for fast
 parallelized computation and training.
----------working with torch text and data
 from torchtext import data
 from torchtext import datasets

 questions = data.Field(tokenize = ‘spacy’, batch_first = True)
 labels = data.LabelField(dtype = torch.float)

 train_data, _ = datasets.TREC.splits(questions, labels)
 train_data, valid_data = train_data.split()

 train_data.examples[0].text


 While it is possible for us to train our own embedding layer, we can instead transform our data
 using the pre-computed glove vectors that we discussed in Chapter 3, Performing Text Embeddings.
  This also has the added benefit of making our model faster to train as we won't manually need to
   train our embedding layer from scratch


   train_iterator, valid_iterator = data.BucketIterator.splits( (train_data, valid_data),
batch_size = 64,
device = device)


glove_embeddings = questions.vocab.vectors
 model.embedding.weight.data.copy_(glove_embeddings)


 In the case of our encoder, the hidden state represents the context vector representation
 of our whole sentence, meaning we can use the hidden state output of our RNN to represent
 the entirety of the input sentence:

in the context of our sequence-to-sequence models, we append "start" and "end"
 tokens to the beginning and end of our input sentence,

Similarly, in the decoder step, we will see that our decoder will
keep generating words until it predicts an "end" token.

However, by using teacher forcing, we train our model using the correct previous target word
 so that one wrong prediction does not inhibit our model's ability to learn from the correct predictions.

 The Multi30k dataset in Torchtext consists of approximately
 30,000 sentences with corresponding translations in multiple languages.

 However, by predicting the first word correctly, we maximize our chances of
  predicting the whole sentence correctly.


  class Encoder(nn.Module):
def __init__(self, input_dims, emb_dims, hid_dims,
super().__init__() self.hid_dims = hid_dims self.n_layers = n_layers
self.embedding = nn.Embedding(input_dims, emb_dims)
self.rnn = nn.LSTM(emb_dims, hid_dims, n_layers, dropout = dropout)
self.dropout = nn.Dropout(dropout)

def forward(self, src):
embedded = self.dropout(self.embedding(src)) outputs, (h, cell) = self.rnn(embedded) return h, cell

lass Decoder(nn.Module):
def __init__(self, output_dims, emb_dims, hid_dims,
super().__init__()
self.output_dims = output_dims self.hid_dims = hid_dims self.n_layers = n_layers
self.embedding = nn.Embedding(output_dims, emb_dims)
self.rnn = nn.LSTM(emb_dims, hid_dims, n_layers, dropout = dropout)
self.fc_out = nn.Linear(hid_dims, output_dims) self.dropout = nn.Dropout(dropout)




OUI le LSTM de pytorch va en fait faire la recurrence sur la sequence lenght que tu lui provide!!
) GRUs have proven to have similar performance levels over short sequences of data as LSTMs. LSTMs
are more useful when learning longer sequences of data. In this instance we are only using input
sentences with 10 words or less, so GRUs should produce similar results.



Denis Rothman - Transformers for Natural Language Processing_ Build innovative deep neural
network architectures for NLP with Python, PyTorch, TensorFlow, BERT, RoBERTa, and more-Packt Publishing Ltd






roBERTa: A variation of BERT, built by Facebook. Removes the next sentence prediction
element of BERT, but enhances the word masking strategy by implementing dynamic masking.


ALBERT: This Google trained model uses its own unique training method called sentence order
prediction. This variation of BERT has been shown to outperform the standard BERT across a number
 of tasks and is now considered state-of-the-art ahead of BERT (illustrating just how quickly
 things can change!).

 The fact that BERT is trained bidirectionally means this single-token generation is not possible;
 however, GPT-2 is not bidirectional, so it only considers previous words in the sentence when making
  predictions, which is why BERT outperforms GPT-2 when predicting missing words within a sentence.

  Robustly Optimized BERT Pretraining Approach (RoBERTa)


  ROBERTA : For example, it does not use WordPiece tokenization but goes down to
  byte-level Byte Pair Encoding (BPE)

  A byte- level tokenizer will break a string or word down into a sub-string
  or sub-word. There are two main advantages among many others:

   Ġ (whitespace)
   ...the tokenizer...
 'Ġthe', 'Ġtoken',   'izer',
 from tokenizers import ByteLevelBPETokenizer
 tokenizer.train(files=paths, vocab_size=52_000, min_frequency=2,
special_tokens=[
"<s>",])
from tokenizers.processors import BertProcessing #adds start and

tokenizer._tokenizer.post_processor = BertProcessing(
    ("</s>", tokenizer.token_to_id("</s>")),
    ("<s>", tokenizer.token_to_id("<s>")),
)
tokenizer.enable_truncation(max_length=512)

tokenizer.encode("The Critique of Pure Reason.").tokens
  ['<s>', 'The', 'ĠCritique', 'Ġof', 'ĠPure', 'ĠReason', '.', '</s>']